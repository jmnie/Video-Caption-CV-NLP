{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import mxnet as mx \n",
    "from mxnet import gluon,autograd,nd\n",
    "import mxnet.ndarray as F\n",
    "from mxnet.gluon.model_zoo import vision\n",
    "from data_loader import videoFolder\n",
    "import utils\n",
    "from option import Options, args_\n",
    "from multiprocessing import cpu_count\n",
    "from network import lstm_net,resnet18_v2\n",
    "from metrics import L2Loss_2, L2Loss_cos\n",
    "import sys\n",
    "#import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    frames = args.frames\n",
    "    caption_length = args.caption_length\n",
    "    glove_file = args.glove_file\n",
    "    \n",
    "    #CPU_COUNT = multiprocessing.cpu_count()\n",
    "    if args.cuda:\n",
    "        ctx = mx.gpu()\n",
    "    else:\n",
    "        ctx = mx.cpu()\n",
    "    \n",
    "    if args.load_pretrain:\n",
    "        pretrain_model = vision.vgg16_bn(pretrained=True,ctx=ctx)\n",
    "        transform = utils.Compose([utils.ToTensor(ctx),\n",
    "                               utils.normalize(ctx),\n",
    "                               utils.extractFeature(ctx,pretrain_model)\n",
    "                             ])\n",
    "    else:\n",
    "        pretrain_model = None\n",
    "        transform = utils.Compose([utils.ToTensor(ctx),\n",
    "                                   utils.normalize(ctx),\n",
    "                                 ])\n",
    "    \n",
    "    target_transform = utils.targetCompose([utils.WordToTensor(ctx)])\n",
    "\n",
    "    train_dataset = videoFolder(args.train_folder,args.train_dict, frames, glove_file, \n",
    "                    caption_length, ctx, transform=transform, target_transform=target_transform)\n",
    "\n",
    "    test_dataset = videoFolder(args.test_folder,args.test_dict, frames, glove_file, \n",
    "                        caption_length, ctx, transform=transform, target_transform=target_transform)\n",
    "\n",
    "    train_loader = gluon.data.DataLoader(train_dataset,batch_size=args.batch_size,\n",
    "                                last_batch='keep',shuffle=True)\n",
    "\n",
    "    test_loader = gluon.data.DataLoader(test_dataset,batch_size=args.batch_size,\n",
    "                                    last_batch='keep',shuffle=False)\n",
    "\n",
    "    loss = L2Loss_2()\n",
    "    #net = lstm_net(frames,caption_length,ctx,pretrained=args.load_pretrain)\n",
    "    net = resnet18_v2(caption_length=caption_length,ctx=ctx)\n",
    "                            \n",
    "            \n",
    "    net.collect_params().initialize(init=mx.initializer.MSRAPrelu(), ctx=ctx)\n",
    "        \n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam',\n",
    "                            {'learning_rate': args.lr})\n",
    "    \n",
    "    smoothing_constant = 0.01\n",
    "    \n",
    "    for e in range(args.epochs):\n",
    "        \n",
    "        epoch_loss = 0.\n",
    "        for batch_id, (x,_) in enumerate(train_loader):\n",
    "                            \n",
    "            with autograd.record():\n",
    "                pred = net(x)\n",
    "                batch_loss = loss(pred,_)\n",
    "            \n",
    "            trainer.step(x.shape[0],ignore_stale_grad=True)\n",
    "            batch_loss.backward()\n",
    "            mx.nd.waitall()\n",
    "            \n",
    "            batch_loss = F.mean(batch_loss,axis=0).asnumpy()[0]\n",
    "            if ((batch_id == 0) and (e == 0)):\n",
    "                epoch_loss = batch_loss\n",
    "            else:\n",
    "                epoch_loss = (1 - smoothing_constant)*epoch_loss + smoothing_constant*batch_loss\n",
    "            \n",
    "            if (batch_id+1) % 100 == 0:\n",
    "                print(\"Train Batch:{}, batch_loss:{}\".format(batch_id+1, batch_loss))\n",
    "                \n",
    "        epoch_loss_1 = 0.\n",
    "        for batch_id, (x,_) in enumerate(test_loader):\n",
    "                            \n",
    "            with autograd.predict_mode():\n",
    "                predict = net(x)\n",
    "                batch_loss_1 = loss(pred,_)\n",
    "                \n",
    "            batch_loss_1 = F.mean(batch_loss_1,axis=0).asnumpy()[0]\n",
    "            #if (batch_id+1) % 30 == 0:\n",
    "            #    print(\"Test Batch:{}, batch_loss:{}\".format(batch_id+1, batch_loss_1))\n",
    "                \n",
    "            if ((batch_id == 0) and (e == 0)):\n",
    "                epoch_loss_1 = batch_loss_1\n",
    "            else:\n",
    "                epoch_loss_1 = (1 - smoothing_constant)*epoch_loss_1 + smoothing_constant*batch_loss_1\n",
    "                \n",
    "            \n",
    "        print(\"Epoch {}, train_loss:{}, test_loss:{}\".format(e+1, epoch_loss, epoch_loss_1))\n",
    "    \n",
    "    if args.save_model == True:\n",
    "        file_name = \"./saved_model/\" + \"lstm_pretrain.params\"\n",
    "        net.save_parameters(file_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = args_()\n",
    "    train(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Batch:100, batch_loss:0.4223302900791168\n",
      "Train Batch:200, batch_loss:0.4086487889289856\n",
      "Train Batch:300, batch_loss:0.3608972728252411\n",
      "Train Batch:400, batch_loss:0.33032113313674927\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Argument data must have NDArray type, but got 0.33709067",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9905f9b930de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-b3ffd1e37315>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-fd38b728f2b0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_id\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mbatch_loss_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_loss_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m                 \u001b[0mepoch_loss_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msmoothing_constant\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mepoch_loss_1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msmoothing_constant\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_loss_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/mxnet/ndarray/register.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(data, axis, keepdims, exclude, out, name, **kwargs)\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Argument data must have NDArray type, but got 0.33709067"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
