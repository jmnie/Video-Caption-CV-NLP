{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import mxnet as mx \n",
    "from mxnet import gluon,autograd,nd\n",
    "import mxnet.ndarray as F\n",
    "from mxnet.gluon.model_zoo import vision\n",
    "from data_loader import videoFolder\n",
    "import utils\n",
    "from option import Options, args_\n",
    "from multiprocessing import cpu_count\n",
    "from network import lstm_net,resnet18_v2\n",
    "from metrics import L2Loss_2, L2Loss_cos\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "#import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    frames = args.frames\n",
    "    caption_length = args.caption_length\n",
    "    glove_file = args.glove_file\n",
    "    \n",
    "    #CPU_COUNT = multiprocessing.cpu_count()\n",
    "    if args.cuda:\n",
    "        ctx = mx.gpu()\n",
    "    else:\n",
    "        ctx = mx.cpu()\n",
    "    \n",
    "    if args.load_pretrain:\n",
    "        pretrain_model = vision.vgg16_bn(pretrained=True,ctx=ctx)\n",
    "        transform = utils.Compose([utils.ToTensor(ctx),\n",
    "                               utils.normalize(ctx),\n",
    "                               utils.extractFeature(ctx,pretrain_model)\n",
    "                             ])\n",
    "    else:\n",
    "        pretrain_model = None\n",
    "        transform = utils.Compose([utils.ToTensor(ctx),\n",
    "                                   utils.normalize(ctx),\n",
    "                                 ])\n",
    "    \n",
    "    target_transform = utils.targetCompose([utils.WordToTensor(ctx)])\n",
    "\n",
    "    train_dataset = videoFolder(args.train_folder,args.train_dict, frames, glove_file, \n",
    "                    caption_length, ctx, transform=transform, target_transform=target_transform)\n",
    "\n",
    "    test_dataset = videoFolder(args.test_folder,args.test_dict, frames, glove_file, \n",
    "                        caption_length, ctx, transform=transform, target_transform=target_transform)\n",
    "\n",
    "    train_loader = gluon.data.DataLoader(train_dataset,batch_size=args.batch_size,\n",
    "                                last_batch='keep',shuffle=True)\n",
    "\n",
    "    test_loader = gluon.data.DataLoader(test_dataset,batch_size=args.batch_size,\n",
    "                                    last_batch='keep',shuffle=False)\n",
    "\n",
    "    loss = L2Loss_2()\n",
    "    net = lstm_net(frames,caption_length,ctx,pretrained=args.load_pretrain)\n",
    "    #net = resnet18_v2(caption_length=caption_length,ctx=ctx)\n",
    "                            \n",
    "            \n",
    "    net.collect_params().initialize(init=mx.initializer.MSRAPrelu(), ctx=ctx)\n",
    "        \n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam',\n",
    "                            {'learning_rate': args.lr})\n",
    "    \n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    train_loss_batch = []\n",
    "    test_loss_batch = []\n",
    "    \n",
    "    smoothing_constant = 0.01\n",
    "    \n",
    "    for e in range(args.epochs):\n",
    "        \n",
    "        epoch_loss = 0.\n",
    "        for batch_id, (x,_) in enumerate(train_loader):\n",
    "                            \n",
    "            with autograd.record():\n",
    "                pred = net(x)\n",
    "                batch_loss = loss(pred,_)\n",
    "            \n",
    "            trainer.step(x.shape[0],ignore_stale_grad=True)\n",
    "            batch_loss.backward()\n",
    "            mx.nd.waitall()\n",
    "            \n",
    "            batch_loss = F.mean(F.mean(batch_loss,axis=1),axis=0).asnumpy()[0]\n",
    "            if ((batch_id == 0) and (e == 0)):\n",
    "                epoch_loss = batch_loss\n",
    "            else:\n",
    "                epoch_loss = (1 - smoothing_constant)*epoch_loss + smoothing_constant*batch_loss\n",
    "            \n",
    "            train_loss_batch.append(batch_loss)\n",
    "            \n",
    "            if (batch_id+1) % 200 == 0:\n",
    "                print(\"Train Batch:{}, batch_loss:{}\".format(batch_id+1, batch_loss))\n",
    "                \n",
    "            if ((e+1)*(batch_id + 1)) % (2*args.log_interval) == 0:\n",
    "                # save model\n",
    "                save_model_filename = \"Epoch_\" + str(e) + \"_iters_\" + str(batch_id + 1) + '_'  + str(time.ctime()).replace(' ', '_') + \"_\" + \".params\"\n",
    "                \n",
    "                save_model_path = os.path.join(args.model_path, save_model_filename)\n",
    "                net.save_parameters(save_model_path)\n",
    "                print(\"\\nCheckpoint, trained model saved at\", save_model_path)\n",
    "                \n",
    "                train_loss_filename = \"Epoch_\" + str(e) + \"_iters_\" + str(batch_id + 1) + str(time.ctime()).replace(' ', '_') + \"_train_loss\" + \".txt\"\n",
    "                \n",
    "                train_loss_path = os.path.join(args.log_path, train_loss_filename)\n",
    "                np.savetxt(train_loss_path,np.array(train_loss_batch))\n",
    "        \n",
    "                \n",
    "        epoch_loss_1 = 0.\n",
    "        for batch_id, (x,_) in enumerate(test_loader):\n",
    "                            \n",
    "            with autograd.predict_mode():\n",
    "                predict = net(x)\n",
    "                batch_loss_1 = loss(pred,_)\n",
    "                \n",
    "            batch_loss_1 = F.mean(F.mean(batch_loss_1,axis=1),axis=0).asnumpy()[0]\n",
    "            #if (batch_id+1) % 30 == 0:\n",
    "            #    print(\"Test Batch:{}, batch_loss:{}\".format(batch_id+1, batch_loss_1))\n",
    "                \n",
    "            if ((batch_id == 0) and (e == 0)):\n",
    "                epoch_loss_1 = batch_loss_1\n",
    "            else:\n",
    "                epoch_loss_1 = (1 - smoothing_constant)*epoch_loss_1 + smoothing_constant*batch_loss_1\n",
    "            \n",
    "            test_loss_batch.append(batch_loss_1)\n",
    "            \n",
    "            if ((e+1)*(batch_id + 1)) % (2*args.log_interval) == 0:\n",
    "                \n",
    "                test_loss_file_name = \"Epoch_\" + str(e) + \"_iters_\" + str(batch_id + 1) + str(time.ctime()).replace(' ', '_') + \"_test_loss\" + \".txt\"\n",
    "                test_loss_path = os.path.join(args.log_path, test_loss_filename)\n",
    "                np.savetxt(test_loss_path,np.array(test_loss_batch))\n",
    "                \n",
    "        \n",
    "        train_loss.append(epoch_loss)\n",
    "        test_loss.append(epoch_loss_1)\n",
    "        \n",
    "        print(\"Epoch {}, train_loss:{}, test_loss:{}\".format(e+1, epoch_loss, epoch_loss_1))\n",
    "     \n",
    "    # save model\n",
    "    save_model_filename = \"Final_epoch_\" + str(args.epochs) + \"_\" + str(time.ctime()).replace(' ', '_') + \"_\" + \".params\"\n",
    "    save_model_path = os.path.join(args.model_path, save_model_filename)\n",
    "    net.save_parameters(save_model_path)\n",
    "    print(\"\\nDone, trained model saved at\", save_model_path)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = args_()\n",
    "    train(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Batch:200, batch_loss:0.1849835216999054\n",
      "Train Batch:400, batch_loss:0.13339275121688843\n",
      "Train Batch:600, batch_loss:0.1715521663427353\n",
      "Train Batch:800, batch_loss:0.14402133226394653\n",
      "Epoch 1, train_loss:0.1714460617680015, test_loss:0.1483125742661218\n",
      "Train Batch:200, batch_loss:0.21429890394210815\n",
      "Train Batch:400, batch_loss:0.16789226233959198\n",
      "Train Batch:600, batch_loss:0.14673668146133423\n",
      "Train Batch:800, batch_loss:0.16172295808792114\n",
      "Epoch 2, train_loss:0.16127907781361567, test_loss:0.18475167715053897\n",
      "Train Batch:200, batch_loss:0.15870104730129242\n",
      "Train Batch:400, batch_loss:0.13638430833816528\n",
      "Train Batch:600, batch_loss:0.15287335216999054\n",
      "Train Batch:800, batch_loss:0.12758323550224304\n",
      "Epoch 3, train_loss:0.14802916017235196, test_loss:0.13134149529460667\n",
      "Train Batch:200, batch_loss:0.14417718350887299\n",
      "Train Batch:400, batch_loss:0.16037681698799133\n",
      "Train Batch:600, batch_loss:0.18147048354148865\n",
      "Train Batch:800, batch_loss:0.11260728538036346\n",
      "Epoch 4, train_loss:0.14812595354678224, test_loss:0.14489814548323376\n",
      "Train Batch:200, batch_loss:0.14804905652999878\n",
      "Train Batch:400, batch_loss:0.1437256634235382\n",
      "Train Batch:600, batch_loss:0.10688597708940506\n",
      "Train Batch:800, batch_loss:0.12137986719608307\n",
      "\n",
      "Checkpoint, trained model saved at ./saved_model/Epoch_4_iters_800_Mon_Apr_22_01:38:47_2019_.params\n",
      "Epoch 5, train_loss:0.1469008854522445, test_loss:0.13294669755765304\n",
      "Train Batch:200, batch_loss:0.14644986391067505\n",
      "Train Batch:400, batch_loss:0.19611665606498718\n",
      "Train Batch:600, batch_loss:0.1477072685956955\n",
      "Train Batch:800, batch_loss:0.16244013607501984\n",
      "Epoch 6, train_loss:0.14628317518134176, test_loss:0.145645801250999\n",
      "Train Batch:200, batch_loss:nan\n",
      "Train Batch:400, batch_loss:nan\n",
      "Train Batch:600, batch_loss:nan\n",
      "Train Batch:800, batch_loss:nan\n",
      "Epoch 7, train_loss:nan, test_loss:nan\n",
      "Train Batch:200, batch_loss:nan\n",
      "Train Batch:400, batch_loss:nan\n",
      "\n",
      "Checkpoint, trained model saved at ./saved_model/Epoch_7_iters_500_Mon_Apr_22_02:30:10_2019_.params\n",
      "Train Batch:600, batch_loss:nan\n",
      "Train Batch:800, batch_loss:nan\n",
      "Epoch 8, train_loss:nan, test_loss:nan\n",
      "Train Batch:200, batch_loss:nan\n",
      "Train Batch:400, batch_loss:nan\n",
      "Train Batch:600, batch_loss:nan\n",
      "Train Batch:800, batch_loss:nan\n",
      "Epoch 9, train_loss:nan, test_loss:nan\n",
      "Train Batch:200, batch_loss:nan\n",
      "Train Batch:400, batch_loss:nan\n",
      "\n",
      "Checkpoint, trained model saved at ./saved_model/Epoch_9_iters_400_Mon_Apr_22_03:00:05_2019_.params\n",
      "Train Batch:600, batch_loss:nan\n",
      "Train Batch:800, batch_loss:nan\n",
      "\n",
      "Checkpoint, trained model saved at ./saved_model/Epoch_9_iters_800_Mon_Apr_22_03:05:27_2019_.params\n",
      "Epoch 10, train_loss:nan, test_loss:nan\n",
      "Train Batch:200, batch_loss:nan\n",
      "Train Batch:400, batch_loss:nan\n",
      "Train Batch:600, batch_loss:nan\n",
      "Train Batch:800, batch_loss:nan\n",
      "Epoch 11, train_loss:nan, test_loss:nan\n",
      "Train Batch:200, batch_loss:nan\n",
      "Train Batch:400, batch_loss:nan\n",
      "Train Batch:600, batch_loss:nan\n",
      "Train Batch:800, batch_loss:nan\n",
      "Epoch 12, train_loss:nan, test_loss:nan\n",
      "Train Batch:200, batch_loss:nan\n",
      "Train Batch:400, batch_loss:nan\n",
      "Train Batch:600, batch_loss:nan\n",
      "Train Batch:800, batch_loss:nan\n",
      "Epoch 13, train_loss:nan, test_loss:nan\n",
      "Train Batch:200, batch_loss:nan\n",
      "Train Batch:400, batch_loss:nan\n",
      "Train Batch:600, batch_loss:nan\n",
      "Train Batch:800, batch_loss:nan\n",
      "Epoch 14, train_loss:nan, test_loss:nan\n",
      "Train Batch:200, batch_loss:nan\n",
      "Train Batch:400, batch_loss:nan\n",
      "Train Batch:600, batch_loss:nan\n",
      "Train Batch:800, batch_loss:nan\n",
      "\n",
      "Checkpoint, trained model saved at ./saved_model/Epoch_14_iters_800_Mon_Apr_22_04:23:31_2019_.params\n",
      "Epoch 15, train_loss:nan, test_loss:nan\n",
      "Train Batch:200, batch_loss:nan\n",
      "\n",
      "Checkpoint, trained model saved at ./saved_model/Epoch_15_iters_250_Mon_Apr_22_04:31:47_2019_.params\n",
      "Train Batch:400, batch_loss:nan\n",
      "\n",
      "Checkpoint, trained model saved at ./saved_model/Epoch_15_iters_500_Mon_Apr_22_04:35:08_2019_.params\n",
      "Train Batch:600, batch_loss:nan\n",
      "\n",
      "Checkpoint, trained model saved at ./saved_model/Epoch_15_iters_750_Mon_Apr_22_04:38:29_2019_.params\n",
      "Train Batch:800, batch_loss:nan\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_loss_filename' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9905f9b930de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-b3ffd1e37315>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-2725f870c383>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                 \u001b[0mtest_loss_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                 \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loss_filename' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
