{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import mxnet as mx \n",
    "from mxnet import gluon,autograd,nd\n",
    "import mxnet.ndarray as F\n",
    "from mxnet.gluon.model_zoo import vision\n",
    "from data_loader import videoFolder\n",
    "import utils\n",
    "from option import Options, args_\n",
    "from multiprocessing import cpu_count\n",
    "from network import lstm_net,resnet18_v2\n",
    "from metrics import L2Loss_2, L2Loss_cos\n",
    "import sys\n",
    "#import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    frames = args.frames\n",
    "    caption_length = args.caption_length\n",
    "    glove_file = args.glove_file\n",
    "    \n",
    "    #CPU_COUNT = multiprocessing.cpu_count()\n",
    "    if args.cuda:\n",
    "        ctx = mx.gpu()\n",
    "    else:\n",
    "        ctx = mx.cpu()\n",
    "    \n",
    "    if args.load_pretrain:\n",
    "        pretrain_model = vision.vgg16_bn(pretrained=True,ctx=ctx)\n",
    "        transform = utils.Compose([utils.ToTensor(ctx),\n",
    "                               utils.normalize(ctx),\n",
    "                               utils.extractFeature(ctx,pretrain_model)\n",
    "                             ])\n",
    "    else:\n",
    "        pretrain_model = None\n",
    "        transform = utils.Compose([utils.ToTensor(ctx),\n",
    "                                   utils.normalize(ctx),\n",
    "                                 ])\n",
    "    \n",
    "    target_transform = utils.targetCompose([utils.WordToTensor(ctx)])\n",
    "\n",
    "    train_dataset = videoFolder(args.train_folder,args.train_dict, frames, glove_file, \n",
    "                    caption_length, ctx, transform=transform, target_transform=target_transform)\n",
    "\n",
    "    test_dataset = videoFolder(args.test_folder,args.test_dict, frames, glove_file, \n",
    "                        caption_length, ctx, transform=transform, target_transform=target_transform)\n",
    "\n",
    "    train_loader = gluon.data.DataLoader(train_dataset,batch_size=args.batch_size,\n",
    "                                last_batch='keep',shuffle=True)\n",
    "\n",
    "    test_loader = gluon.data.DataLoader(test_dataset,batch_size=args.batch_size,\n",
    "                                    last_batch='keep',shuffle=False)\n",
    "\n",
    "    loss = L2Loss_2()\n",
    "    #net = lstm_net(frames,caption_length,ctx,pretrained=args.load_pretrain)\n",
    "    net = resnet18_v2(caption_length=caption_length,ctx=ctx)\n",
    "                            \n",
    "            \n",
    "    net.collect_params().initialize(init=mx.initializer.MSRAPrelu(), ctx=ctx)\n",
    "        \n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam',\n",
    "                            {'learning_rate': args.lr})\n",
    "    \n",
    "    smoothing_constant = 0.01\n",
    "    \n",
    "    for e in range(args.epochs):\n",
    "        \n",
    "        epoch_loss = 0.\n",
    "        batch_loss = None\n",
    "        for batch_id, (x,_) in enumerate(train_loader):\n",
    "            \n",
    "            if batch_id > 0:\n",
    "                batch_loss = F.mean(batch_loss).asscalar()\n",
    "                epoch_loss = (1 - smoothing_constant)*epoch_loss + smoothing_constant*batch_loss\n",
    "                \n",
    "            with autograd.record():\n",
    "                pred = net(x)\n",
    "                batch_loss = loss(pred,_)\n",
    "            \n",
    "            trainer.step(x.shape[0],ignore_stale_grad=True)\n",
    "            batch_loss.backward()\n",
    "            mx.nd.waitall()\n",
    "            \n",
    "            \n",
    "            if (batch_id+1) % 100 == 0:\n",
    "                print(\"Train Batch:{}, batch_loss:{}\".format(batch_id+1, epoch_loss))\n",
    "            \n",
    "            if ((batch_id == 0) and (e == 0)):\n",
    "                epoch_loss = F.mean(batch_loss).asscalar() \n",
    "        \n",
    "        \n",
    "        epoch_loss_1 = 0.\n",
    "        batch_loss_1 = None\n",
    "        for batch_id, (x,_) in enumerate(test_loader):\n",
    "            \n",
    "            if batch_id > 0:\n",
    "                batch_loss_1 = F.mean(batch_loss_1).asscalar()\n",
    "                epoch_loss_1 = (1 - smoothing_constant)*epoch_loss_1 + smoothing_constant*batch_loss_1\n",
    "                \n",
    "            with autograd.predict_mode():\n",
    "                predict = net(x)\n",
    "                batch_loss_1 = loss(pred,_)\n",
    "                batch_loss_1 = F.mean(batch_loss_1).asscalar()\n",
    "            \n",
    "            if (batch_id+1) % 30 == 0:\n",
    "                print(\"Test Batch:{}, batch_loss:{}\".format(batch_id+1, epoch_loss_1))\n",
    "                \n",
    "            if ((batch_id == 0) and (e == 0)):\n",
    "                epoch_loss_1 = F.mean(batch_loss_1).asscalar() \n",
    "            \n",
    "        print(\"Epoch {}, train_loss:{}, test_loss:{}\".format(e+1, epoch_loss, epoch_loss_1))\n",
    "    \n",
    "    if args.save_model == True:\n",
    "        file_name = \"./saved_model/\" + \"lstm_pretrain.params\"\n",
    "        net.save_parameters(file_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = args_()\n",
    "    train(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Batch:100, batch_loss:\n",
      "[[0.39366227 0.60824007 0.5983646  0.39073762 0.42150462 0.60927516\n",
      "  0.5869195  0.5564809  0.41579407 0.5152232  0.6547611  0.34676555\n",
      "  0.4387939  0.54638207 0.38036242 0.58439356 0.5669514  0.44710204\n",
      "  0.52117604 0.38693878 0.43803266 0.57516897 0.5460846  0.03833161\n",
      "  0.03758459 0.04205515 0.04178111 0.04087343 0.06362057 0.04280103\n",
      "  0.04354157 0.03852938 0.03785402 0.04787131 0.0475389  0.04072666\n",
      "  0.0436152  0.04702995 0.04028513 0.05011647 0.04889257 0.03809907\n",
      "  0.04242947 0.04639535 0.0390443  0.04616857 0.0415615  0.03838789\n",
      "  0.04260176 0.03609623]\n",
      " [0.35816112 0.64503205 0.6041003  0.46471447 0.40931603 0.3947672\n",
      "  0.43165192 0.39253947 0.40674305 0.6065319  0.61919993 0.0729438\n",
      "  0.08432725 0.06431431 0.06078013 0.06530359 0.08433639 0.08252046\n",
      "  0.06449606 0.06821509 0.0721042  0.06522253 0.05353156 0.05542986\n",
      "  0.06322928 0.07251178 0.05665568 0.05991264 0.08324494 0.06545715\n",
      "  0.0619426  0.05372094 0.05515889 0.05401799 0.06190415 0.05729917\n",
      "  0.06355428 0.06973625 0.04762234 0.07539064 0.06730173 0.05933119\n",
      "  0.05995165 0.06535587 0.05422395 0.07057359 0.06181903 0.04810297\n",
      "  0.0602713  0.04993891]\n",
      " [0.47879082 0.56730103 0.55204505 0.39127204 0.41950473 0.60621125\n",
      "  0.38769072 0.50664157 0.6921959  0.43425652 0.53234977 0.52252084\n",
      "  0.3449678  0.48650664 0.5626376  0.6440051  0.4070879  0.37455562\n",
      "  0.55534273 0.04545021 0.0469903  0.04750924 0.03876264 0.03923339\n",
      "  0.04270508 0.04129941 0.04193424 0.0425829  0.06434564 0.04418189\n",
      "  0.04306307 0.03713278 0.03668194 0.04755878 0.04912008 0.0424845\n",
      "  0.04360207 0.04717154 0.03987232 0.05172764 0.04890964 0.03810576\n",
      "  0.04180015 0.04579993 0.03589678 0.04702776 0.03907322 0.03826108\n",
      "  0.04671771 0.03854415]\n",
      " [0.6066373  0.4797984  0.4861686  0.43338516 0.54679716 0.4104368\n",
      "  0.4944836  0.34251878 0.5887785  0.58679426 0.32935497 0.5866012\n",
      "  0.62083066 0.44332418 0.5144607  0.05701512 0.0755937  0.06819183\n",
      "  0.06051251 0.05447043 0.06848609 0.06245235 0.04535667 0.04827658\n",
      "  0.05175454 0.06514884 0.05210532 0.05106255 0.07241836 0.05427156\n",
      "  0.0530844  0.04296411 0.04319879 0.0488035  0.05890002 0.05316533\n",
      "  0.05324224 0.06196871 0.04181255 0.06691038 0.06381008 0.0517368\n",
      "  0.04522514 0.05665134 0.0562536  0.06315538 0.05588972 0.0524168\n",
      "  0.05616517 0.04586577]\n",
      " [0.39640757 0.55827695 0.42856824 0.42811748 0.64181495 0.36642724\n",
      "  0.4725889  0.3926884  0.3794174  0.5519577  0.35032302 0.42628193\n",
      "  0.54035676 0.488407   0.04825947 0.04845246 0.05551924 0.05968644\n",
      "  0.04441218 0.04509664 0.04853832 0.04672666 0.03968051 0.04080646\n",
      "  0.04296875 0.04199232 0.04233676 0.0437373  0.06604654 0.04596884\n",
      "  0.04347161 0.03981189 0.03908619 0.04856462 0.04992817 0.0419552\n",
      "  0.04484181 0.0501148  0.03849499 0.05170937 0.04646509 0.04002326\n",
      "  0.04280237 0.04484732 0.04184778 0.04719111 0.04366414 0.04049618\n",
      "  0.04468533 0.03809746]\n",
      " [0.5503638  0.52829176 0.45259255 0.6422637  0.4887     0.6668917\n",
      "  0.44341677 0.68530935 0.43031174 0.5798228  0.4706706  0.4470649\n",
      "  0.5488297  0.49420273 0.40252265 0.5880031  0.58888936 0.48468614\n",
      "  0.49465    0.46949846 0.7483337  0.26244533 0.20463789 0.22205001\n",
      "  0.20865852 0.26374146 0.22719453 0.23716064 0.28996646 0.27573508\n",
      "  0.21539202 0.19601925 0.23911905 0.2317233  0.22635399 0.25986975\n",
      "  0.24448825 0.26027182 0.19114412 0.28570613 0.29681855 0.23835944\n",
      "  0.26782924 0.2526186  0.22985457 0.32402247 0.24102041 0.20278959\n",
      "  0.26926786 0.20114878]\n",
      " [0.3937427  0.45005196 0.47885746 0.49445134 0.48040202 0.5348788\n",
      "  0.42718923 0.41658744 0.64716166 0.3536115  0.3328991  0.48370802\n",
      "  0.42025363 0.58317137 0.39069617 0.35583234 0.42920223 0.4765415\n",
      "  0.42224246 0.56846434 0.05602017 0.05531955 0.04050054 0.04119405\n",
      "  0.04713728 0.04864807 0.04499854 0.04205166 0.06621782 0.05142437\n",
      "  0.05125301 0.04200606 0.04219915 0.0472756  0.05573531 0.04822446\n",
      "  0.04723426 0.05678998 0.03182782 0.05597329 0.05500005 0.04481453\n",
      "  0.05322266 0.04898984 0.04446605 0.05096066 0.04868269 0.04790388\n",
      "  0.05163889 0.04216664]\n",
      " [0.6024592  0.56734395 0.33502772 0.434504   0.5444035  0.36589366\n",
      "  0.46594208 0.5456078  0.3424674  0.38366482 0.43854627 0.38391382\n",
      "  0.51560557 0.4849753  0.05082047 0.05510754 0.06859061 0.06656919\n",
      "  0.05670414 0.0496845  0.06067639 0.05550766 0.04104732 0.04054125\n",
      "  0.05094633 0.05449032 0.04660329 0.04363762 0.07155832 0.05447473\n",
      "  0.05372469 0.04195705 0.04283436 0.04718857 0.06237881 0.04739701\n",
      "  0.05271139 0.05919601 0.04603939 0.06388197 0.06193522 0.05624875\n",
      "  0.05063532 0.05133445 0.04830754 0.05597322 0.05138906 0.04351296\n",
      "  0.0511188  0.04479275]\n",
      " [0.39474118 0.49320993 0.40960786 0.4361801  0.5137777  0.34524807\n",
      "  0.42402732 0.5608468  0.4212796  0.5427825  0.57189673 0.41389507\n",
      "  0.43807724 0.39948085 0.42052907 0.4312521  0.39372638 0.56636053\n",
      "  0.5053848  0.58271074 0.05590139 0.05567416 0.04194732 0.03846833\n",
      "  0.04514955 0.0476511  0.0446764  0.04553157 0.06886156 0.04682851\n",
      "  0.05104864 0.04161665 0.04227625 0.04971758 0.05018301 0.04269176\n",
      "  0.05073946 0.05527935 0.04369582 0.05296075 0.05299941 0.04314583\n",
      "  0.04627629 0.04677114 0.04444056 0.05118357 0.04937686 0.04885147\n",
      "  0.04605062 0.03599405]\n",
      " [0.3924717  0.54062444 0.53466535 0.38137108 0.42139113 0.58614165\n",
      "  0.47981694 0.38751435 0.4832521  0.3831052  0.38759533 0.575447\n",
      "  0.05399429 0.04784722 0.04578298 0.04848699 0.05652788 0.05399606\n",
      "  0.04742397 0.04369865 0.0490418  0.04899963 0.03765368 0.04000923\n",
      "  0.03870191 0.04360362 0.0436475  0.04138565 0.06452478 0.04225063\n",
      "  0.04313356 0.03745502 0.03643055 0.04687789 0.04770055 0.0440019\n",
      "  0.04051196 0.04879083 0.03813364 0.05449386 0.0499321  0.03755312\n",
      "  0.0433414  0.04565933 0.03832063 0.04788912 0.04309998 0.03721794\n",
      "  0.04607262 0.03691393]\n",
      " [0.3597607  0.6090025  0.6377496  0.4325669  0.36433083 0.48347867\n",
      "  0.5364017  0.39734283 0.3395087  0.6961758  0.59056926 0.5498938\n",
      "  0.56654346 0.6319064  0.47415474 0.51203936 0.44590333 0.39797837\n",
      "  0.51836544 0.45762658 0.39835653 0.5785146  0.36861825 0.60415864\n",
      "  0.39348346 0.36786756 0.42807946 0.5585848  0.06685244 0.0463459\n",
      "  0.04860924 0.0355674  0.04187061 0.04624736 0.05001773 0.04639082\n",
      "  0.04945731 0.05837174 0.0352051  0.05526213 0.05476968 0.0442759\n",
      "  0.04627017 0.05018609 0.04648238 0.05260564 0.04929014 0.04383849\n",
      "  0.04785338 0.03609569]\n",
      " [0.4009518  0.50982565 0.49892005 0.502847   0.46878406 0.53877866\n",
      "  0.5835265  0.4065107  0.51657337 0.4792923  0.4051852  0.4938142\n",
      "  0.6651325  0.66884005 0.18876898 0.19043249 0.23460743 0.218421\n",
      "  0.16285782 0.17784618 0.21363421 0.1936939  0.17111628 0.16137438\n",
      "  0.15954888 0.19105923 0.15980537 0.16771454 0.25657123 0.19234253\n",
      "  0.1611147  0.16398288 0.19155338 0.17446041 0.19606239 0.18089554\n",
      "  0.18358985 0.20060425 0.166908   0.23190813 0.21602803 0.18588482\n",
      "  0.17092039 0.1826813  0.16002074 0.22626539 0.16724196 0.15409198\n",
      "  0.1719128  0.16190964]\n",
      " [0.5371202  0.487759   0.42797035 0.41825837 0.48436913 0.40648788\n",
      "  0.49372143 0.48388758 0.3632817  0.4120549  0.5893055  0.4149931\n",
      "  0.5181388  0.39413688 0.5550333  0.4277129  0.62335616 0.11349627\n",
      "  0.09240935 0.10438095 0.14097697 0.11723471 0.08063818 0.09473821\n",
      "  0.09578193 0.12359361 0.09975874 0.11333702 0.1213919  0.1092611\n",
      "  0.11161324 0.10358473 0.09395092 0.09646109 0.10688264 0.10445619\n",
      "  0.09980196 0.12035919 0.09396376 0.12236953 0.1293725  0.0899955\n",
      "  0.10510737 0.10242105 0.08695912 0.12922646 0.10668706 0.09937082\n",
      "  0.11841297 0.08298377]\n",
      " [0.39075768 0.5569149  0.3886198  0.7081599  0.5620413  0.55888647\n",
      "  0.61993307 0.37128615 0.67845744 0.47077248 0.39982826 0.06390485\n",
      "  0.06266878 0.05339248 0.05106083 0.05625769 0.06780967 0.06906841\n",
      "  0.05457535 0.04855184 0.0592834  0.05813104 0.04481487 0.0425617\n",
      "  0.04986753 0.05549647 0.04585501 0.03957081 0.06953365 0.04942311\n",
      "  0.04914173 0.04097259 0.04720686 0.05474835 0.0476868  0.04610575\n",
      "  0.04967431 0.0581423  0.0424813  0.06037163 0.05413895 0.04294492\n",
      "  0.049435   0.05041781 0.04281642 0.0566902  0.05407378 0.04134221\n",
      "  0.04991236 0.04266262]\n",
      " [0.36734393 0.48383263 0.48361287 0.38039905 0.656084   0.37233844\n",
      "  0.41118094 0.4766859  0.3430342  0.62009454 0.48259202 0.49880278\n",
      "  0.5628527  0.39264023 0.47273403 0.47984365 0.5858128  0.12671997\n",
      "  0.10559245 0.10013895 0.11866145 0.1091963  0.0941949  0.08292404\n",
      "  0.10792179 0.10988515 0.09181752 0.08920595 0.13780612 0.10669316\n",
      "  0.10897961 0.08453546 0.10551431 0.09520824 0.10609556 0.10244684\n",
      "  0.10341083 0.1148257  0.08724555 0.12672138 0.12673596 0.09954976\n",
      "  0.11317849 0.11050857 0.09377299 0.1365957  0.10401259 0.0937595\n",
      "  0.10116362 0.08761916]\n",
      " [0.3798368  0.4333624  0.59436285 0.5000116  0.42315617 0.44435218\n",
      "  0.37972695 0.36758965 0.34377098 0.4993199  0.35486642 0.43538192\n",
      "  0.51353765 0.4437366  0.44919717 0.4080319  0.3899191  0.5533064\n",
      "  0.07784717 0.07430647 0.08285701 0.08177805 0.05509344 0.06754187\n",
      "  0.06931216 0.08046648 0.06678464 0.06093447 0.09840801 0.07220259\n",
      "  0.07486356 0.05973429 0.07070893 0.0648807  0.08232774 0.06984912\n",
      "  0.07469776 0.08723948 0.05809547 0.08717585 0.0884757  0.06929822\n",
      "  0.06943915 0.07189328 0.06580219 0.08039942 0.07382981 0.05957752\n",
      "  0.07434668 0.06341199]]\n",
      "<NDArray 16x50 @gpu(0)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9905f9b930de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-b3ffd1e37315>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-d6608b0256a9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mignore_stale_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36mwaitall\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0mThis\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbenchmarking\u001b[0m \u001b[0monly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \"\"\"\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0mcheck_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMXNDArrayWaitAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
